{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2 - Classifying images of microstructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some modules that we will require\n",
    "import numpy as np  \n",
    "import scipy as sp\n",
    "import cv2 # This is an image processing module\n",
    "#import pandas as pd\n",
    "import sklearn # This is a module of machine learning tools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We thought it might be fun to use our new-found knowledge of machine learning to tackle a real, current research problem. A hot-topic in materials science, both in academia and industry, is how we might make use of machine learning and the tools of Big Data in discovering new materials. An important challenge in this field is how we deal with the sort of data that we get from experiments, which can be noisy and shows large variations. In this notebook we will show how the tools in scikit-learn allow us to automatically classify images of microstructures of different types.\n",
    "\n",
    "The method we will use is taken from a recent research article:\n",
    "\n",
    "![Paper header](Images/DecostHolmHeader.jpg \"Title\")\n",
    "\n",
    "I have implemented it especially for this course. You can read more about it in the original paper, but I will also describe it briefly below. It's a bit involved, but bear with me and it should become clearer as we work through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method works as follows:\n",
    "\n",
    "1. We begin with a set of micrographs covering the range of microstructures we wish to distinguish.\n",
    "2. We use a common image *descriptor* to detect features of interest in this set of images.\n",
    "3. We use an *unsupervised* learning method to detect distinct clusters of these descriptors. The centres of these clusters then form the \"words\" in a visual dictionary that we can use to describe images of micorstructure.\n",
    "4. We now return to our set of micrographs, but this time we assign them to different classes. \n",
    "5. For each micrograph, we apply the image descriptor to find the features of interest and classify them according to our previously determined clusters. Each feature is thus assigned to a given \"word\" within our visual dictionary.\n",
    "6. For each micrograph we now count up the occurance of each word. The histogram of word frequency becomes the visual \"fingerprint\" of thre micrograph.\n",
    "7. We now use a *supervised* learning method to train a classifier to relate the fingerprints of our training set to the classes of microstructure.\n",
    "8. We test and evaluate the performance of our classifier on an unseen test sample of micrographs.\n",
    "\n",
    "Now let's see how this works out in practice....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Some images to work with\n",
    "To keep things simple we will stick to a binary classification between two types of microstructure: a super-alloy type and a basket-weave microstructure. I have grabbed a series of suitable images from the internet. The first thing to do is build a list of the images for the training and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = ['BasketWeave','Superalloy']\n",
    "ninfolder = [11,9]\n",
    "folderstub = ['BW', 'SA']\n",
    "ntest = [3,3]\n",
    "nfoldersuse = 2\n",
    "\n",
    "files = []\n",
    "folderid = []\n",
    "for s in range(nfoldersuse):\n",
    "    for t in range(ntest[s],ninfolder[s]):\n",
    "        files.append('Micrographs/' + folders[s] + '/' + folderstub[s] + str(t+1) + '.jpg')\n",
    "        folderid.append(s)\n",
    "nfiles = len(files)\n",
    "\n",
    "testfiles = []\n",
    "testfolderid = []\n",
    "for s in range(nfoldersuse):\n",
    "    for t in range(ntest[s]):\n",
    "        testfiles.append('Micrographs/' + folders[s] + '/' + folderstub[s] + str(t+1) + '.jpg')\n",
    "        testfolderid.append(s)\n",
    "ntestfiles = len(testfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some typical examples (look in the folder if you wish to view the others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 5\n",
    "nimages = 14\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, dpi=300)\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "for i in range(nimages):\n",
    "    img = mpimg.imread(files[i])\n",
    "    axes[int((i-i%ncols)/ncols), i%ncols].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though we are simplifiying our task by considering only a binary classification there is still quite a lot of variation in the images in terms of the image qualtity, the scale of the features, etc. This is still quite a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Applying the SIFT descriptor\n",
    "The SIFT descriptor is a method of describing features on interest in an image based on the local gradients of intensity at the feature. An important charcteristic of the SIFT descriptor is that it is invariant with scaling and rotation of an image. A common use for SIFT descriptors is tracking features between frames in a movie. \n",
    "\n",
    "The cell below applies feature detection and illustrates the sift descriptor for a sample image. Note that we always begin by making a greyscale version of the micrograph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(files[0])\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp, des = sift.detectAndCompute(gray,None)\n",
    "img=cv2.drawKeypoints(gray,kp,img,flags=4)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detectAndCompute()` method of a `sift`object returns an array of sift descriptors for the image. Each descriptor consists of 128 numbers characterising the variation in intensity of the image around the feature. The `drawKeypoints()` method helpfully adds an indication of the postion and scale of the features to our image. Now let's examine the data returned for the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(kp), type(des))\n",
    "print(np.shape(kp))\n",
    "print(np.shape(des))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detectAndCompute()` method returns a list of the positions, sizes and orientations of the key points (the features), which we assign to the variable `kp` and an array of descriptors, which we assign to `des`. We can see that this image contains 995 features of interest.\n",
    "\n",
    "Now lets use a `for` loop to generate descriptors for all of our training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "pts = []\n",
    "imgid = []\n",
    "msid = []\n",
    "\n",
    "for s in range(nfiles):\n",
    "    #print(files[s])\n",
    "    img = cv2.imread(files[s])\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "    #print(len(des))\n",
    "    for r in range(len(des)):\n",
    "        features.append(des[r,:].tolist())\n",
    "        #pts.append([kp[r].pt[0],kp[r].pt[1],kp[r].size,kp[r].angle])\n",
    "        #imgid.append(s)\n",
    "        #msid.append(folderid[s])\n",
    "\n",
    "features = np.array(features)\n",
    "#pts = np.array(pts)\n",
    "#msid = np.array(msid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for our inital unsupervised clustering, all we care about is forming a huge list of all the features gathered from the training data. We do not care at this stage about which microstructures the come from. We can check how many descriptors we have to work with by looking at the size of the `features` array that we have built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Unsupervised learning to build the \"bag of words\"\n",
    "Our next step is to carry out an unsupervised clustering of the array of descriptors to try to extract characteristic features that will form the \"words\". To do this we are going to use the k-mean clustering algorithm that we started out this session by investigating. The choice of the number of clusters is the key parameter here and I had to play around with it a little beofre settling on a value of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "nclusters = 30\n",
    "kmeans = KMeans(n_clusters=nclusters)\n",
    "kmeans.fit(features[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means clustering places the large number of SIFT descriptors extracted from our images into 30 clusters. The centres of these clusters define 30 \"words, which each consist of 128 numbers (you could think fo them as vectors in a 128-dimensional parameter space, if you wish.\n",
    "\n",
    "Let's take a quick look at the first word in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very informative, but you should get the general idea that we now have these lists of numbers (that we call visual \"words\") which we can use to describe microstructures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the set of training images\n",
    "## Step 5: Apply the image descriptor and clustering to the images\n",
    "## Step 6: Build the \"fingerprints\" of the training images\n",
    "The next three steps will all be performed at once. We define a loop to iterate over all the images in the training set. For each image, we hunt for features and calculate the SIFT descriptors for each (juat as we did earlier). But now we have a trained k-means classifier that we use to assign each feature to one of the 30 words in our dictionary. We then count up the number of occurnaces of each word and normalise by the total count to give a histogram of words in each image that is our \"fingerprint\" of that image.\n",
    "\n",
    "Let's take a look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up arrays to hold the training inputs and outputs\n",
    "trainx = np.zeros((nfiles,nclusters),dtype=float)\n",
    "trainy = np.zeros(nfiles, dtype=int)\n",
    "\n",
    "# Iterate over images in training set\n",
    "for s in range(nfiles):\n",
    "    features = []\n",
    "    img = cv2.imread(files[s])\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray,None)  # Detect features and calculate SIFT descriptors\n",
    "    for r in range(len(des)):\n",
    "        features.append(des[r,:].tolist())\n",
    "    features = np.array(features)\n",
    "    labels = kmeans.predict(features[:,:])     # Use k-means cluster model to assign features to clusters (\"words\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    # Build a histogram of occurences of words - the fingerprint of the image\n",
    "    for t in range(len(unique)):\n",
    "        trainx[s,unique[t]] = counts[t]\n",
    "        trainy[s] = folderid[s]\n",
    "    trainx[s,:] = trainx[s,:]/np.sum(trainx[s,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us understand what we have done, lets take a look at the fingerprint of the first image in our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainx[0,:])\n",
    "plt.xlabel('Word index')\n",
    "plt.ylabel('Relative frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little tricky to see how this random looking plot might encode details of the microstructure. Let's compare the fingerprints for all the training images and see if there are obvious differences between the tow types of microstructure that we are studying. \n",
    "\n",
    "At the moment, the words in the fingerprint are in a random order. To make this comparison clear bewtween microstructures clearer, we will reorder the words so that ones that are more common in the superalloy microstructure are early in the list and those that are more common in the basket-weave microstructure are near the end. We can get python to work out this ordering. (See if you can understand what this code is doing, but don't worry if not as it is just to help with viusalising what is going on and is not part of the machine learning process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orderscore = np.sum(trainx[:ninfolder[0]-ntest[0],:],axis=0) - np.sum(trainx[ninfolder[0]-ntest[0]:ninfolder[0]-ntest[0]+ninfolder[1]-ntest[1],:],axis=0)\n",
    "orderid = np.argsort(orderscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "nrows = 4\n",
    "nplots = 8\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, dpi=100)\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "for i in range(nplots):\n",
    "    #print(int((i-i%ncols)/ncols), i%ncols,plotidx)\n",
    "    axes[int((i-i%ncols)/ncols), i%ncols].plot(trainx[i,orderid])\n",
    "    \n",
    "fig.subplots_adjust(hspace=0)\n",
    "fig.suptitle('Fingerprints for basket-weave', fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "nrows = 3\n",
    "nplots = 6\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, dpi=100)\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "for i in range(nplots):\n",
    "    #print(int((i-i%ncols)/ncols), i%ncols,plotidx)\n",
    "    axes[int((i-i%ncols)/ncols), i%ncols].plot(trainx[i+8,orderid])\n",
    "    \n",
    "fig.subplots_adjust(hspace=0)\n",
    "fig.suptitle('Fingerprints for super-alloy', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does seem to be some sytematic difference in the fingerprints for the two microstructures, so there is hope that machine learning amy be able to pick up on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use supervised learning to train a SVM to classify fingerprints of images\n",
    "We now use a *supervised learning* approach to train a classifier based on our training images. Once again, we will adopt an approach that we have already tried and use a support vector machine. The code to train this SVM is below. Once again, I needed to do a bit of fine tuning of the parameters `C` and `gamma`, but not how simple this is: essentially one line is needed to train the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "\n",
    "svm = SVC(kernel=chi2_kernel, C=1.0, gamma=1.0).fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test performance of SVM classifier on set of unseen test images\n",
    "All that remains now is to test the SVM classifier on our unseen images. To do this, we need to load the images, determine the SIFT descriptors for the features in the images. Use the trained k-means clustering (from our unsupervised learning) to assign words to the desriptors, count the occurences of the words to build the fingerprints of the images and then use the trained SVM (form our supervised learning) to predict the micorstructure of the image. Let's do this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx = np.zeros((ntestfiles,nclusters),dtype=float)\n",
    "testy = np.zeros(ntestfiles, dtype=int)\n",
    "predicty = np.zeros(ntestfiles, dtype=int)\n",
    "for s in range(ntestfiles):\n",
    "    features = []\n",
    "    img = cv2.imread(testfiles[s])\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray,None)\n",
    "    for r in range(len(des)):\n",
    "        features.append(des[r,:].tolist())\n",
    "    features = np.array(features)\n",
    "    labels = kmeans.predict(features[:,:])\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for t in range(len(unique)):\n",
    "        testx[s,unique[t]] = counts[t]\n",
    "        testy[s] = testfolderid[s]\n",
    "    testx[s,:] = testx[s,:]/np.sum(testx[s,:])\n",
    "    # Predict here!!\n",
    "    labels = svm.predict(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 2\n",
    "nimages = 6\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, dpi=300)\n",
    "for ax in axes.flat:\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "for i in range(nimages):\n",
    "    img = mpimg.imread(testfiles[i])\n",
    "    axes[int((i-i%ncols)/ncols), i%ncols].imshow(img)\n",
    "    axes[int((i-i%ncols)/ncols), i%ncols].set_title(\"Basket-weave\" if labels[i]==0 else \"Super-alloy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
